{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as skmetrics\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, precision_recall_fscore_support\nimport torch\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import EarlyStoppingCallback","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-28T18:56:07.905022Z","iopub.execute_input":"2023-03-28T18:56:07.905762Z","iopub.status.idle":"2023-03-28T18:56:07.912054Z","shell.execute_reply.started":"2023-03-28T18:56:07.905724Z","shell.execute_reply":"2023-03-28T18:56:07.910814Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/natural-disaster/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-03-28T18:51:48.545180Z","iopub.execute_input":"2023-03-28T18:51:48.545917Z","iopub.status.idle":"2023-03-28T18:51:48.576026Z","shell.execute_reply.started":"2023-03-28T18:51:48.545875Z","shell.execute_reply":"2023-03-28T18:51:48.574934Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T18:51:58.821592Z","iopub.execute_input":"2023-03-28T18:51:58.822574Z","iopub.status.idle":"2023-03-28T18:52:00.796440Z","shell.execute_reply.started":"2023-03-28T18:51:58.822516Z","shell.execute_reply":"2023-03-28T18:52:00.795255Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\nModel config BertConfig {\n  \"_name_or_path\": \"bert-base-uncased\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\nModel config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}\n\nloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"text = list(data[\"text\"])\ntarget = list(data[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-28T18:52:07.511602Z","iopub.execute_input":"2023-03-28T18:52:07.512098Z","iopub.status.idle":"2023-03-28T18:52:07.527027Z","shell.execute_reply.started":"2023-03-28T18:52:07.512039Z","shell.execute_reply":"2023-03-28T18:52:07.525946Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Разделяет выборку на train, val, test.\nx_train, x_rest, y_train, y_rest = train_test_split(text, target, test_size=0.2)\n\nx_val, x_test, y_val, y_test = train_test_split(x_rest, y_rest, test_size=0.5)\n\n# Получает токены сообщений, а также приводит к одному размеру - 512 токенов.\nx_train_tokenized = tokenizer(x_train, padding=True, truncation=True, max_length=512)\nx_val_tokenized = tokenizer(x_val, padding=True, truncation=True, max_length=512)\nx_test_tokenized = tokenizer(x_test, padding=True, truncation=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T20:02:44.093647Z","iopub.execute_input":"2023-03-28T20:02:44.094032Z","iopub.status.idle":"2023-03-28T20:02:49.508750Z","shell.execute_reply.started":"2023-03-28T20:02:44.093996Z","shell.execute_reply":"2023-03-28T20:02:49.507674Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2023-03-28T20:04:30.855318Z","iopub.execute_input":"2023-03-28T20:04:30.855762Z","iopub.status.idle":"2023-03-28T20:04:30.864156Z","shell.execute_reply.started":"2023-03-28T20:04:30.855722Z","shell.execute_reply":"2023-03-28T20:04:30.863046Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(x_train_tokenized, y_train)\nval_dataset = Dataset(x_val_tokenized, y_val)\ntest_dataset = Dataset(x_test_tokenized, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T20:04:38.313484Z","iopub.execute_input":"2023-03-28T20:04:38.313983Z","iopub.status.idle":"2023-03-28T20:04:38.325365Z","shell.execute_reply.started":"2023-03-28T20:04:38.313933Z","shell.execute_reply":"2023-03-28T20:04:38.323965Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"execution":{"iopub.status.busy":"2023-03-28T20:04:43.828812Z","iopub.execute_input":"2023-03-28T20:04:43.830709Z","iopub.status.idle":"2023-03-28T20:04:43.845354Z","shell.execute_reply.started":"2023-03-28T20:04:43.830629Z","shell.execute_reply":"2023-03-28T20:04:43.843853Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=\"output\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    seed=0,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T21:00:30.512132Z","iopub.execute_input":"2023-03-28T21:00:30.512527Z","iopub.status.idle":"2023-03-28T21:00:30.520827Z","shell.execute_reply.started":"2023-03-28T21:00:30.512492Z","shell.execute_reply":"2023-03-28T21:00:30.519751Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-28T21:02:53.699945Z","iopub.execute_input":"2023-03-28T21:02:53.700649Z","iopub.status.idle":"2023-03-28T21:02:53.713487Z","shell.execute_reply.started":"2023-03-28T21:02:53.700612Z","shell.execute_reply":"2023-03-28T21:02:53.712436Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-28T21:02:56.581518Z","iopub.execute_input":"2023-03-28T21:02:56.581893Z","iopub.status.idle":"2023-03-28T21:08:22.658043Z","shell.execute_reply.started":"2023-03-28T21:02:56.581858Z","shell.execute_reply":"2023-03-28T21:08:22.657042Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num examples = 6090\n  Num Epochs = 5\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 955\n  Number of trainable parameters = 109483778\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='955' max='955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [955/955 05:25, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.065500</td>\n      <td>0.884343</td>\n      <td>0.817346</td>\n      <td>0.783151</td>\n      <td>0.784375</td>\n      <td>0.781931</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"***** Running Evaluation *****\n  Num examples = 761\n  Batch size = 32\nSaving model checkpoint to output/checkpoint-500\nConfiguration saved in output/checkpoint-500/config.json\nModel weights saved in output/checkpoint-500/pytorch_model.bin\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from output/checkpoint-500 (score: 0.8843425512313843).\n","output_type":"stream"},{"execution_count":157,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=955, training_loss=0.05029099848882066, metrics={'train_runtime': 326.0471, 'train_samples_per_second': 93.391, 'train_steps_per_second': 2.929, 'total_flos': 1314424721484000.0, 'train_loss': 0.05029099848882066, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_trainer = Trainer(model)\n\n# Получает предсказания\nraw_pred, _, _ = test_trainer.predict(test_dataset)\n\n# Переводит предсказания в [0, 1]\ny_pred = np.argmax(raw_pred, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-28T21:10:03.349787Z","iopub.execute_input":"2023-03-28T21:10:03.350402Z","iopub.status.idle":"2023-03-28T21:10:07.178696Z","shell.execute_reply.started":"2023-03-28T21:10:03.350353Z","shell.execute_reply":"2023-03-28T21:10:07.177654Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stderr","text":"No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n***** Running Prediction *****\n  Num examples = 762\n  Batch size = 16\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"print(\"Bert-based-model accuracy:\", accuracy_score(y_pred, y_test))\nprint(\"Precision\", precision_score(y_pred, y_test))\nprint(\"Recall\", recall_score(y_pred, y_test))\nprint(\"F1 Score:\", f1_score(y_pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-03-28T21:10:09.412696Z","iopub.execute_input":"2023-03-28T21:10:09.413116Z","iopub.status.idle":"2023-03-28T21:10:09.428263Z","shell.execute_reply.started":"2023-03-28T21:10:09.413061Z","shell.execute_reply":"2023-03-28T21:10:09.427047Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"Bert-based-model accuracy: 0.8097112860892388\nPrecision 0.7225609756097561\nRecall 0.8144329896907216\nF1 Score: 0.7657512116316639\n","output_type":"stream"}]}]}